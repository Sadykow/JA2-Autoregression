\ifthenelse{\boolean{thesis}}{
An optimal set of hyperparameters has been obtained based on early results in Chapter~\ref{cha:Analysis}, Section~\ref{sec:AN:Results}.
} {
An optimal set of hyperparameters has been obtained based on early results in previous work~\cite{sadykov_practical_2022}.
}
However, the number of outputs from a model, referred to as output steps, had to be experimentally tested and compared prior to the primary training procedure.
After that, ten models of a single layer with 131 neurons LSTM with Autoregression were trained on all provided temperature ranges ten times, validated against 25\textdegree{}C of its' profile, and tested against 25 \& 30\textdegree{}C of the other two profiles.
An average of all ten results were plotted and later tested against entire training sets to measure the performance and minimise the stochastic effect.

\subsection{Optimal output step size}
    The optimal size of the Autoregression output steps $AR-width$ for better capturing the SoC was determined through experimental training.
    \mbox{Table~\ref{tab:out_steps}} outlines the training iterations of each driving profile until it reaches accuracy below 2.5\%
    \ifthenelse{\boolean{thesis}}{, and Figure~\ref{fig:Models_res} plots the final best results on a feed-forward test. } {.}
    % The training continued for as long as there was no overfitting determined.
    During $AR-width$ evaluation through several attempts, no overfitting has been observed in on testing to date.
    Profiles which already reached an error below the predetermined error were excluded from the followed up experiments.
    Those that did not reach 2.5\% were prevented from more epoch training since the increase of $AR-width$ by five more samples is more time convenient than waiting for the error to diverge.
    Because of that, even with 25 steps on FUDS dataset providing a reasonable accuracy, the increase towards 30 lowered the percentage below the initially expected and reached it through fewer epochs.
    \ifthenelse{\boolean{thesis}}{
    \begin{table}[htbp]
        \renewcommand{\arraystretch}{1.3}
        \caption{Percentage accuracy evaluation with increasing output step size}
        \centering
        \label{tab:out_steps}
        \begin{tabular}{ c | c c c | r }
            \hline\hline \\[-4mm]
            AR-width & DST & US06 & FUDS & Epochs \\
            \hline
            10 & 2.94 & $\geq\sim 15$ & $\geq\sim 15$ & 8 \\
            15 & 0.90 & 10.67 & 11.10 & 8 \\
            20 & -- & 0.68  &  4.25 & 10 \\
            25 & -- & --  &  3.48 & 25 \\
            30 & -- & --  &  0.59 & 19 \\
            \hline\hline
        \end{tabular}
    \end{table}
    }{
    \begin{table}[htbp]
        \renewcommand{\arraystretch}{1.3}
        \caption{Percentage accuracy evaluation with increasing output step size}
        \centering
        \label{tab:out_steps}
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{ c | c c c | r }
            \hline\hline \\[-4mm]
            AR-width & DST & US06 & FUDS & Epochs \\
            \hline
            10 & 2.94 & $\geq\sim 15$ & $\geq\sim 15$ & 8 \\
            15 & 0.90 & 10.67 & 11.10 & 8 \\
            20 & -- & 0.68  &  4.25 & 10 \\
            25 & -- & --  &  3.48 & 25 \\
            30 & -- & --  &  0.59 & 19 \\
            \hline\hline
        \end{tabular}
        }
    \end{table}
    }
    
%
%
\ifthenelse{\boolean{thesis}}{
The Dynamic Stress Test has a stable trend of current consumption and regenerative process.
Therefore it did not take long to capture the behaviour of the charge.
A single cycle on \mbox{subfigure~\ref{subfig:res_DST}} provides an example of accurate feed-forward charge estimation with output steps between 10-15 samples.
The US06 and FUDS have more aggressive current usage, leading to a higher step $s$ number.
As per \mbox{subfigures~\ref{subfig:res_US} and~\ref{subfig:res_FUDS}}, it took twice as many output samples to achieve similar accuracy to the DST-based model.
Any further increase can capture even more complicated scenarios, leading to a longer training time.
However, every added step decreased the epoch per training model to produce the best fit.
\begin{figure}[htbp]
    \centering
    % DST based tests
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        % \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-15steps/DST-models/SMRDST-FF-8.svg}
        \includesvg[width=\linewidth]{III_Conclussion/im_steps/SMRDST-FF-8.svg}
        \caption{Best DST based model after 8 iterations with 15 steps}
        \label{subfig:res_DST}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        % \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-20steps/US06-models/SMRUS06-FF-10.svg}
        \includesvg[width=\linewidth]{III_Conclussion/im_steps/SMRUS06-FF-10.svg}
        \caption{Best US06 based model after 10 iterations with 20 steps}
        \label{subfig:res_US}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        % \includesvg[width=\linewidth]{III_Conclussion/im_best/FUDS-19.svg}
        % \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-FF-19.svg}
        \includesvg[width=\linewidth]{III_Conclussion/im_steps/SMRFUDS-FF-19.svg}
        \caption{Best FUDS based model after 19 iterations with 30 steps}
        \label{subfig:res_FUDS}
    \end{subfigure}
    \caption{Best training results over 3 different driving profiles}
    \label{fig:Models_res}
\end{figure}
%
%
An optimal step is required for running the training process on the entire dataset with all three profiles as inputs to prepare for future deployment on hardware.
Therefore, all other results demonstrations will use 30 steps as the most optimal and report any other results based on that parameter.
} { 
Because all drive cycles were suitable accurate after 30 steps of $AR-width$,
the same value will be used for all further models.
Despite making the training process slow compared to other steps due to concurrent looping through additional outputs and feed-forward, the time for producing a single sample at the output is the same.
}

\subsection{Accuracy comparison}
With cross-training and testing over ten attempts average (to minimise the stochastic nature of the Machine Learning), Figure~\ref{fig:Model-6res} has been produced to demonstrate the resulting predictions.
The total errors for each model are summarised in Table~\ref{tab:acc-results2}.
It shows results from the most accurate model in~\cite{sadykov_practical_2022}, models 1 \& 3, compared to present results.
In total, 30 models were produced in this study, averaged, and compared for the best usage in an Electric Vehicle.
\ifthenelse{\boolean{thesis}}{
\begin{figure*}[htbp]
    \centering
    %%%%%%%%%%%%%%%%% DST based tests %%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/M6-history-DST-mae.svg}
        \caption{Average training and testing MAE history average of 10 attempts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/DST-6-train.svg}
        \caption{Validation on a single DST cycle of SoC estimation average of 10 attempts at 25\textdegree{}C}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/DST-6-test.svg}
        \caption{Testing on two cycles of US06 and FUDS profiles average of 10 attempts}
        \label{subfig:Model-6res-DSTvsFUDS}
    \end{subfigure}
    %%%%%%%%%%%%%%%%% US06 based tests %%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/M6-history-US06-mae.svg}
        \caption{Average training and testing MAE history average of 10 attempts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/US06-6-train.svg}
        \caption{Validation on a single US06 cycle of SoC estimation average of 10 attempts at 25\textdegree{}C}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/US06-6-test.svg}
        \caption{Testing on two cycles of DST and FUDS profiles average of 10 attempts}
    \end{subfigure}
    % %%%%%%%%%%%%%%%%% FUDS based tests %%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/M6-history-FUDS-mae.svg}
        \caption{Average training and testing MAE history average of 10 attempts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/FUDS-6-train.svg}
        \caption{Validation on a single FUDS cycle of SoC estimation average of 10 attempts at 25\textdegree{}C}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{II_Body/images/FUDS-6-test.svg}
        \caption{Testing on two cycles of DST and US06 profiles average of 10 attempts}
    \end{subfigure}
    \caption{Model 6: Stateless LSTM with 30 steps of Autoregression.}
    \label{fig:Model-6res}
\end{figure*}
} {
\begin{figure*}[!h]
    \centering
    %%%%%%%%%%%%%%%%% DST based tests %%%%%%%%%%%%%%%%%
    \subfloat[Average training and testing MAE history; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/M6-history-DST-mae.svg}}
    \hfill
    \subfloat[Validation on a single DST cycle of SoC estimation; average of 10 attempts at 25\textdegree{}C]{\includesvg[width=0.325\linewidth]{II_Body/images/DST-6-train.svg}}
    \hfill
    \subfloat[Testing on two cycles of US06 and FUDS profiles; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/DST-6-test.svg}}
    \label{subfig:Model-6res-DSTvsFUDS}
    %%%%%%%%%%%%%%%%% US06 based tests %%%%%%%%%%%%%%%%%
    \subfloat[Average training and testing MAE history; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/M6-history-US06-mae.svg}}
    \hfill
    \subfloat[Validation on a single US06 cycle of SoC estimation; average of 10 attempts at 25\textdegree{}C]{\includesvg[width=0.325\linewidth]{II_Body/images/US06-6-train.svg}}
    \subfloat[Testing on two cycles of DST and FUDS profiles; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/US06-6-test.svg}}
    % %%%%%%%%%%%%%%%%% FUDS based tests %%%%%%%%%%%%%%%%%
    \hfill
    \subfloat[Average training and testing MAE history; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/M6-history-FUDS-mae.svg}}
    \hfill
    \subfloat[Validation on a single FUDS cycle of SoC estimation; average of 10 attempts at 25\textdegree{}C]{\includesvg[width=0.325\linewidth]{II_Body/images/FUDS-6-train.svg}}
    \hfill
    \subfloat[Testing on two cycles of DST and US06 profiles; average of 10 attempts]{\includesvg[width=0.325\linewidth]{II_Body/images/FUDS-6-test.svg}}
    \caption{Model 6: Stateless LSTM with 30 steps of Autoregression.}
    \label{fig:Model-6res}
\end{figure*}
}
\ifthenelse {\boolean{thesis}}
{
\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.3}
    \caption{Accuracy result summary for entire training datasets between best existing models from Chapter~\ref{cha:Analysis} and novel method.}
    \centering
    \label{tab:acc-results2}
\resizebox{\textwidth}{!}{
\begin{tabular}{ c| l| c c c| c c c |c c c}
    \hline\hline \\[-4mm]
% Columns setup
    \multirow{3}{1em}{\#} &
    \multirow{3}{3em}{Trained} &
    \multicolumn{9}{c}{Tested} \\
    \cline{3-11}
    & & 
    \multicolumn{3}{c|}{DST} &
    \multicolumn{3}{c|}{US06} &
    \multicolumn{3}{c}{FUDS} \\
    \cline{3-11}
     & & MAE(\%) & RMSE(\%) & $R^{2}$(\%) & MAE(\%) & RMSE(\%) & $R^{2}$(\%) & MAE(\%) & RMSE(\%) & $R^{2}$(\%) \\
    \hline
    % Content
    %Chemali2017
      & DST & 2.77 & 3.52 & 98.71 & 2.86 & 3.93 & 98.34 & 3.28 & 4.62 & 97.66 \\ 
    1 & US06 & 5.97 & 7.97 & 93.39 & 3.37 & 4.14 & 98.15 & 5.38 & 6.93 & 94.73 \\ 
      & FUDS & 5.03 & 7.26 & 94.51 & 4.02 & 6.07 & 96.04 & 1.95 & 2.85 & 99.11 \\ 
    \hline
      & DST & 2.86 & 3.60 & 98.65 & 2.91 & 3.79 & 98.46 & 3.73 & 5.18 & 97.06 \\ 
    3 & US06 & 5.98 & 8.26 & 92.90 & 3.35 & 4.11 & 98.19 & 5.27 & 6.84 & 94.87 \\ 
      & FUDS & 5.33 & 7.25 & 94.53 & 3.61 & 5.53 & 96.71 & 1.82 & 2.51 & 99.31 \\ 
    % GelarehJavid2020
    % \hline
    %   & DST & 2.89 & 3.61 & 98.65 & 3.82 & 5.38 & 96.88 & 4.11 & 5.51 & 96.67 \\ 
    % 4 & US06 & 6.19 & 8.57 & 92.35 & 3.30 & 4.12 & 98.17 & 5.42 & 6.82 & 94.91 \\ 
    %   & FUDS & 5.74 & 7.49 & 94.16 & 4.03 & 5.75 & 96.44 & 1.58 & 2.28 & 99.43 \\ 
    \hline
    \textbf{New} & \textbf{DST} & \textbf{0.81} & \textbf{1.10} & \textbf{99.87} & \textbf{1.92} & \textbf{2.71} & \textbf{99.21} & \textbf{1.77} & \textbf{2.35} & \textbf{99.40}  \\
    \textbf{Met-}& \textbf{US06} & \textbf{1.49} & \textbf{2.02} & \textbf{99.57} & \textbf{0.89} & \textbf{1.06} & \textbf{99.88} & \textbf{1.28} & \textbf{1.94} & \textbf{99.59}  \\
    \textbf{hod} & \textbf{FUDS} & \textbf{2.10} & \textbf{2.94} & \textbf{99.10} & \textbf{1.28} & \textbf{1.87} & \textbf{99.62} & \textbf{0.50} & \textbf{0.68} & \textbf{99.95}  \\
    \hline\hline
\end{tabular}
}
\end{table}
} {
\begin{table*}[!ht]
    \renewcommand{\arraystretch}{1.3}
    \caption{Accuracy result summary for entire training datasets. 
    Models 1 and 3 were taken from previous work~\cite{sadykov_practical_2022}.}
    \centering
    \label{tab:acc-results2}
\resizebox{\linewidth}{!}{
    \begin{tabular}{ c l c c c c c c c c c}
        \hline\hline \\[-4mm]
    % Columns setup
        \multirow{4}{*}{\textbf{\#}\vspace{+8pt}} &
        \multirow{4}{*}{\textbf{Trained}\vspace{+8pt}} &
        \multicolumn{9}{c}{\textbf{Tested}} \\
        \cline{3-11}
        & & 
        \multicolumn{3}{c}{\textbf{DST}} &
        \multicolumn{3}{c}{\textbf{US06}} &
        \multicolumn{3}{c}{\textbf{FUDS}} \\
        \cline{3-11}
        & & \textbf{MAE~(\%)} & \textbf{RMSE~(\%)} & \textbf{\boldmath{$R^{2}$~(\%)}} & \textbf{MAE~(\%)} & \textbf{RMSE~(\%)} & \textbf{\boldmath{$R^{2}$~(\%)}} & \textbf{MAE~(\%)} & \textbf{RMSE~(\%)} & \textbf{\boldmath{$R^{2}$~(\%)}} \\
    \hline
        \textbf{New} & \textbf{DST} & \textbf{0.81} & \textbf{1.10} & \textbf{99.87} & \textbf{1.92} & \textbf{2.71} & \textbf{99.21} & \textbf{1.77} & \textbf{2.35} & \textbf{99.40}  \\
        \textbf{Met-}& \textbf{US06} & \textbf{1.49} & \textbf{2.02} & \textbf{99.57} & \textbf{0.89} & \textbf{1.06} & \textbf{99.88} & \textbf{1.28} & \textbf{1.94} & \textbf{99.59}  \\
        \textbf{hod} & \textbf{FUDS} & \textbf{2.10} & \textbf{2.94} & \textbf{99.10} & \textbf{1.28} & \textbf{1.87} & \textbf{99.62} & \textbf{0.50} & \textbf{0.68} & \textbf{99.95}  \\
    \hline
    % Content
    %Chemali2017
        & DST & 2.77 & 3.52 & 98.71 & 2.86 & 3.93 & 98.34 & 3.28 & 4.62 & 97.66 \\ 
    1~\cite{sadykov_practical_2022} & US06 & 5.97 & 7.97 & 93.39 & 3.37 & 4.14 & 98.15 & 5.38 & 6.93 & 94.73 \\ 
        & FUDS & 5.03 & 7.26 & 94.51 & 4.02 & 6.07 & 96.04 & 1.95 & 2.85 & 99.11 \\ 
    \hline
        & DST & 2.86 & 3.60 & 98.65 & 2.91 & 3.79 & 98.46 & 3.73 & 5.18 & 97.06 \\ 
    3~\cite{sadykov_practical_2022} & US06 & 5.98 & 8.26 & 92.90 & 3.35 & 4.11 & 98.19 & 5.27 & 6.84 & 94.87 \\ 
        & FUDS & 5.33 & 7.25 & 94.53 & 3.61 & 5.53 & 96.71 & 1.82 & 2.51 & 99.31 \\ 
    % GelarehJavid2020
    % \hline
    %   & DST & 2.89 & 3.61 & 98.65 & 3.82 & 5.38 & 96.88 & 4.11 & 5.51 & 96.67 \\ 
    % 4 & US06 & 6.19 & 8.57 & 92.35 & 3.30 & 4.12 & 98.17 & 5.42 & 6.82 & 94.91 \\ 
    %   & FUDS & 5.74 & 7.49 & 94.16 & 4.03 & 5.75 & 96.44 & 1.58 & 2.28 & 99.43 \\ 
    \hline\hline
\end{tabular}
}
\end{table*}
}

%!
%!
%! Find a better place
\begin{figure*}[!h]
    \centering
    %%%%%%%%%%%%%%%%% DST based tests %%%%%%%%%%%%%%%%%
    \subfloat[New methods' variance visualisation]{\includesvg[width=0.325\linewidth]{III_Conclussion/im_var/M6-variance.svg}}
    \hfill
    \subfloat[A simple LSTM models' variance visualisation]{\includesvg[width=0.325\linewidth]{III_Conclussion/im_var/M1-variance.svg}}
    \hfill
    \subfloat[LSTM with Attention models' variance visualisation]{\includesvg[width=0.325\linewidth]{III_Conclussion/im_var/M3-variance.svg}}
    \label{subfig:Models-var}
\end{figure*}

%
%
Based on plot observation, all profiles achieved an error of around 1\% at the training, which can be observed on feed-forward plots on the validation set.
Most inaccuracies come from small regions of charge, which could be interpreted as a side effect of involving high temperatures in training, causing little offsets between idle and high-intensive consumption.
The testing plot of all three profiles showed similar results to validation, with a slight error increase.
Nevertheless, the results do not deviate from validation as much as similar models the experiment was compared with.

%
%
\ifthenelse{\boolean{thesis}}{
%In addition to the results of the currently-researched model, Table~\ref{tab:acc-results2} includes the best results from Chapter~\ref{cha:Analysis} LSTM models, which are considered as the two best models for charge- discharge SoC estimation on other profiles.
Compared to the previous results of Chapter 3, where the 3-layer model was shown to be the best performing, here, the same amount of neurons in one layer but with autoregression can be seen to be significantly better than any previously-reported variations.
}
{%In addition to the results of the currently-researched model, Table~\ref{tab:acc-results2} includes the LSTM models from previous works~\cite{sadykov_practical_2022}, which were considered the two best models for charge-discharge SoC estimation on other profiles.
When compared to the previous results of~\cite{sadykov_practical_2022} where the 3-layer model was shown to be the best performing, here, the same amount of neurons in one layer but with autoregression can be seen to be significantly better than any previously-reported variations.
}
With the same trained and test profiles, the new method reduced the error compared to LSTM (Model \#1) and LSTM with Attention (Model \#3) by a factor of 3.
In addition, it also reduced testing errors by a factor of 2 against other profiles.
FUDS is the best model to train on and capture complex driving behaviours for the new model.
The conclusion differs from the tests earlier~\cite{sadykov_practical_2022}, where DST was considered the best model to train on and capture the behaviour of other models.
The newly-proposed model produced better capture of the US06 dataset.
Such results confirm that weight priority has been switched from the voltage curve on Model \#1 and \#3 to either current or most likely-known SoC features.
Therefore, since the SoC is a function of current over time, the charge of US06 makes a good generalisation between chaotic urban driving and the most straightforward discharge pulse test profiles.

% The newly-proposed training technique was compared against a similar implementation without a modified training loop.
% Both models were provided with ideal initial results, including the State of Charge.
% Every further prediction replaced the actual charge value and was used as an input in the following input set, along with actual Voltage, Current and Temperature.
\ifthenelse {\boolean{thesis}}
{    
Since electric vehicles may not always start with the initial full capacity of the batteries, several additional plots on Figure~\ref{fig:init_time} were produced to validate models' adaptivity.
Assuming that the initial known SoC has either been recorded from the previous car state or determined by other means, like 3-feature-based models from Chapter~\ref{cha:Analysis}, the new method has little issue with starting condition.
\begin{figure*}[htbp]
    \centering
    % DST based tests
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{III_Conclussion/im_time/train-iCharging.svg}
        \caption{Half-way through charging process}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{III_Conclussion/im_time/train-iCharged.svg}
        \caption{Fully charged state \\ \ \ \ }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includesvg[width=\linewidth]{III_Conclussion/im_time/train-iDischarging.svg}
        \caption{Half-way through discharging process}
    \end{subfigure}
    % \begin{subfigure}[b]{0.475\textwidth}
    %     \centering
    %     \includesvg[width=\linewidth]{III_Conclussion/im_time/train-iDischarged.svg}
    %     \caption{Completely discharged state}
    % \end{subfigure}
    \caption{Different initial periods of model validation}
    \label{fig:init_time}
\end{figure*}
} { }
    %
    %
    % The offset in the validation can be explained by the amount of weight placed into the known SoC, unlike with 3-feature models, where voltages act as the primary characteristic.
    % In the State of Charge estimation, the significant impact is affected by current, and since the State of charge is the function of current and time, the weight the properly applied to the correct feature over the training process.
    % Any other training process appeared to be unnecessary and may lead to overfitting.
    % \mbox{Figure~\ref{fig:Models_res}} demonstrates the best feed-forward prediction for all three profiles.
    %
    %
    % Several plots in \mbox{Figure~\ref{fig:diff_prof_compare}} outline demonstrates the process of model validation and testing.
    % The model has been trained and validated against FUDS-profile, \mbox{subfigure~\ref{subfig:FUDS_diff_prof_compare}}, and tested again DST on \mbox{subfigure~\ref{subfig:DST_diff_prof_compare}} and US06 on \mbox{subfigure~\ref{subfig:US_diff_prof_compare}}.
    % The plots are based on the latest output from the training loop.
    % The entire training process has been logged, and every model is saved as a separate checkpoint to determine the most efficient model in both validation and testing.
    % \mbox{Figure~\ref{fig:res_performance}} demonstrates selecting the best model, marking the iteration which produced the lowest error for all three profiles.
    % This way, the selection process can determine the iteration at which the model reached the lowest error across all three profiles. 
    % As a result, after eight iterations, the model achieved the most optimal results, \mbox{Figure~\ref{fig:diff_prof_best}}.
    % Any further training increases the accuracy over one set but leads to loss of generalisation over the other two.
    % Plots above outline different initial starting points and show the convergence over time. \textcolor{red}{Stupid idea, that's not helpful. How about I add some red line at very beginning, indicated where did I start initially and how far it plotted itself. Accumulate 500 initials steps and then plot them seperately on top to show the indication.}
    % \begin{figure*}[htbp]
    %     \centering
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-FF-19.svg}
    %         \caption{FUDS trained model}
    %         \label{subfig:FUDS_diff_prof_compare}
    %     \end{subfigure}
    %     \hfill
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-Test One-19.svg}
    %         \caption{Testing against DST profile}
    %         \label{subfig:DST_diff_prof_compare}
    %     \end{subfigure}
    %     \hfill
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-Test Two-19.svg}
    %         \caption{Testing against US06 profile}
    %         \label{subfig:US_diff_prof_compare}
    %     \end{subfigure}
    %     \caption{The result of prediction over DST and US06 at the latest iteration}
    %     \label{fig:diff_prof_compare}
    % \end{figure*}
    % \begin{itemize}
    %     \item To verify the efficiency, model also was compared against the two other profiles cycling profiles. \\
    %     \item \textbf{model has tendency to put a lot of weight into the Voltage. This way. weights will fall into SoC instead to preserve longer dependency.} \\
    %     \item \textit{Different comparison, plots. Relative timing and what mechanism in implementation with lists and tensors increased it. Accuracies.} \\
    %     \item \textbf{Comaprison of different Windows size. Time it takes to compute that is too long. Need to overcome it, somehow.} \\
    % \end{itemize}
    % \begin{figure*}
    %     \centering
    %     \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-performance.svg}
    %     \caption{Accuracy evolution over training process for Mean Average Error (MAE) and Root Mean Squared Error (RMSE)}
    %     \label{fig:res_performance}
    % \end{figure*}
    % \begin{figure*}[htbp]
    %     \centering
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/FUDS-FF-8.svg}
    %         \caption{FUDS trained model}
    %         \label{subfig:FUDS_diff_prof_best}
    %     \end{subfigure}
    %     \hfill
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-Test One-8.svg}
    %         \caption{Testing against DST profile}
    %         \label{subfig:DST_diff_prof_best}
    %     \end{subfigure}
    %     \hfill
    %     \begin{subfigure}[b]{0.325\textwidth}
    %         \centering
    %         \includesvg[width=\linewidth]{III_Conclussion/Models/Sadykov2021-30steps/FUDS-models/SMRFUDS-Test Two-8.svg}
    %         \caption{Testing against US06 profile}
    %         \label{subfig:US_diff_prof_best}
    %     \end{subfigure}
    %     \caption{The result of prediction over DST and US06 at the lowest training and validation iteration}
    %     \label{fig:diff_prof_best}
    % \end{figure*}

